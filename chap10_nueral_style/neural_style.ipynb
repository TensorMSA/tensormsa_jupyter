{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neural-style-tf\n",
    "\n",
    "### 뉴로스타일알고리즘은 CNN을 사용하여 원본과 대상 이미지의 화풍(Style)과 내용(Content)을 분리하고 합침으로써 모방한 그림을 합성한다.\n",
    "\n",
    "![lion](./image_content/neural_sytle_first.JPG)\n",
    "\n",
    "### 다른 그림들\n",
    "![lion](./image_content/neural_sytle_second.JPG)\n",
    "\n",
    " [The Shipwreck of the Minotaur](http://www.artble.com/artists/joseph_mallord_william_turner/paintings/the_shipwreck_of_the_minotaur), [The Starry Night](https://www.wikiart.org/en/vincent-van-gogh/the-starry-night-1889), [Composition VII](https://www.wikiart.org/en/wassily-kandinsky/composition-vii-1913), [The Scream](https://www.wikiart.org/en/edvard-munch/the-scream-1893), [Seated Nude](http://www.pablopicasso.org/seated-nude.jsp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Neural-Style-Network 설명\n",
    "![cnn](./image_content/nueral_network_cnn.JPG)\n",
    "\n",
    "#### 스타일 전송 알고리즘의 이해\n",
    " - 왼쪽에 Style 이미지가 CNN VGG19로 모델로 전송하여 Style계산\n",
    " - 오른쪽에 Content 이미지가 CNN VGG19 모델로 전송하여 Content 계산\n",
    " - white Noise가 가득찬 이미지를 가운데 CNN VGG19로 전송하여 Style과 Content 계산\n",
    " - Style, Conetent로 나뉘어 제곱합을 Loss 펑션으로 정의후 최소값(Gradient descent) \n",
    "   찾아 이미지 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural-Style-Network Requisite\n",
    "\n",
    "\n",
    "Download the VGG-19 model weights \n",
    " \n",
    "wget http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat\n",
    "\n",
    "![VGG19](./image_content/VGG19.JPG)\n",
    "\n",
    "[VGG19 ImageNet winner 2014 ](http://www.image-net.org/challenges/LSVRC/2014/results#clsloc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmpocj0xf9v\n",
      "\n",
      "---- RENDERING SINGLE IMAGE ----\n",
      "\n",
      "\n",
      "BUILDING VGG-19 NETWORK\n",
      "loading model weights...\n",
      "constructing layers...\n",
      "LAYER GROUP 1\n",
      "--conv1_1 | shape=(1, 320, 240, 64) | weights_shape=(3, 3, 3, 64)\n",
      "--relu1_1 | shape=(1, 320, 240, 64) | bias_shape=(64,)\n",
      "--conv1_2 | shape=(1, 320, 240, 64) | weights_shape=(3, 3, 64, 64)\n",
      "--relu1_2 | shape=(1, 320, 240, 64) | bias_shape=(64,)\n",
      "--pool1   | shape=(1, 160, 120, 64)\n",
      "LAYER GROUP 2\n",
      "--conv2_1 | shape=(1, 160, 120, 128) | weights_shape=(3, 3, 64, 128)\n",
      "--relu2_1 | shape=(1, 160, 120, 128) | bias_shape=(128,)\n",
      "--conv2_2 | shape=(1, 160, 120, 128) | weights_shape=(3, 3, 128, 128)\n",
      "--relu2_2 | shape=(1, 160, 120, 128) | bias_shape=(128,)\n",
      "--pool2   | shape=(1, 80, 60, 128)\n",
      "LAYER GROUP 3\n",
      "--conv3_1 | shape=(1, 80, 60, 256) | weights_shape=(3, 3, 128, 256)\n",
      "--relu3_1 | shape=(1, 80, 60, 256) | bias_shape=(256,)\n",
      "--conv3_2 | shape=(1, 80, 60, 256) | weights_shape=(3, 3, 256, 256)\n",
      "--relu3_2 | shape=(1, 80, 60, 256) | bias_shape=(256,)\n",
      "--conv3_3 | shape=(1, 80, 60, 256) | weights_shape=(3, 3, 256, 256)\n",
      "--relu3_3 | shape=(1, 80, 60, 256) | bias_shape=(256,)\n",
      "--conv3_4 | shape=(1, 80, 60, 256) | weights_shape=(3, 3, 256, 256)\n",
      "--relu3_4 | shape=(1, 80, 60, 256) | bias_shape=(256,)\n",
      "--pool3   | shape=(1, 40, 30, 256)\n",
      "LAYER GROUP 4\n",
      "--conv4_1 | shape=(1, 40, 30, 512) | weights_shape=(3, 3, 256, 512)\n",
      "--relu4_1 | shape=(1, 40, 30, 512) | bias_shape=(512,)\n",
      "--conv4_2 | shape=(1, 40, 30, 512) | weights_shape=(3, 3, 512, 512)\n",
      "--relu4_2 | shape=(1, 40, 30, 512) | bias_shape=(512,)\n",
      "--conv4_3 | shape=(1, 40, 30, 512) | weights_shape=(3, 3, 512, 512)\n",
      "--relu4_3 | shape=(1, 40, 30, 512) | bias_shape=(512,)\n",
      "--conv4_4 | shape=(1, 40, 30, 512) | weights_shape=(3, 3, 512, 512)\n",
      "--relu4_4 | shape=(1, 40, 30, 512) | bias_shape=(512,)\n",
      "--pool4   | shape=(1, 20, 15, 512)\n",
      "LAYER GROUP 5\n",
      "--conv5_1 | shape=(1, 20, 15, 512) | weights_shape=(3, 3, 512, 512)\n",
      "--relu5_1 | shape=(1, 20, 15, 512) | bias_shape=(512,)\n",
      "--conv5_2 | shape=(1, 20, 15, 512) | weights_shape=(3, 3, 512, 512)\n",
      "--relu5_2 | shape=(1, 20, 15, 512) | bias_shape=(512,)\n",
      "--conv5_3 | shape=(1, 20, 15, 512) | weights_shape=(3, 3, 512, 512)\n",
      "--relu5_3 | shape=(1, 20, 15, 512) | bias_shape=(512,)\n",
      "--conv5_4 | shape=(1, 20, 15, 512) | weights_shape=(3, 3, 512, 512)\n",
      "--relu5_4 | shape=(1, 20, 15, 512) | bias_shape=(512,)\n",
      "--pool5   | shape=(1, 10, 8, 512)\n",
      "\n",
      "MINIMIZING LOSS USING: L-BFGS OPTIMIZER\n",
      "WARNING:tensorflow:From <ipython-input-10-556159189f7f>:632 in minimize_with_lbfgs.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS EXCEEDS LIMIT'\n",
      "  Objective function value: 923732416.000000\n",
      "  Number of iterations: 151\n",
      "  Number of functions evaluations: 163\n",
      "Single image elapsed time: 569.5014038085938\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import scipy.io  \n",
    "import argparse \n",
    "import struct\n",
    "import errno\n",
    "import time                       \n",
    "import cv2\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "'''\n",
    "  parsing and configuration\n",
    "'''\n",
    "def parse_args():\n",
    "\n",
    "  desc = \"TensorFlow implementation of 'A Neural Algorithm for Artisitc Style'\"  \n",
    "  parser = argparse.ArgumentParser(description=desc)\n",
    "\n",
    "  # options for single image\n",
    "  parser.add_argument('--verbose', action='store_true',default='true',\n",
    "    help='Boolean flag indicating if statements should be printed to the console.')\n",
    "\n",
    "  parser.add_argument('--img_name', type=str, \n",
    "    default='result',\n",
    "    help='Filename of the output image.')\n",
    "\n",
    "  parser.add_argument('--style_imgs', nargs='+', type=str,\n",
    "    help='Filenames of the style images (example: starry-night.jpg)', \n",
    "    required=True)\n",
    "  \n",
    "  parser.add_argument('--style_imgs_weights', nargs='+', type=float,\n",
    "    default=[1.0],\n",
    "    help='Interpolation weights of each of the style images. (example: 0.5 0.5)')\n",
    "  \n",
    "  parser.add_argument('--content_img', type=str,\n",
    "    help='Filename of the content image (example: lion.jpg)')\n",
    "\n",
    "  parser.add_argument('--style_imgs_dir', type=str,\n",
    "    default='./styles',\n",
    "    help='Directory path to the style images. (default: %(default)s)')\n",
    "\n",
    "  parser.add_argument('--content_img_dir', type=str,\n",
    "    default='./image_input',\n",
    "    help='Directory path to the content image. (default: %(default)s)')\n",
    "  \n",
    "  parser.add_argument('--init_img_type', type=str, \n",
    "    default='content',\n",
    "    choices=['random', 'content', 'style'], \n",
    "    help='Image used to initialize the network. (default: %(default)s)')\n",
    "  \n",
    "  parser.add_argument('--max_size', type=int, \n",
    "    default=512,\n",
    "    help='Maximum width or height of the input images. (default: %(default)s)')\n",
    "  \n",
    "  parser.add_argument('--content_weight', type=float, \n",
    "    default=5e0,\n",
    "    help='Weight for the content loss function. (default: %(default)s)')\n",
    "  \n",
    "  parser.add_argument('--style_weight', type=float, \n",
    "    default=1e4,\n",
    "    help='Weight for the style loss function. (default: %(default)s)')\n",
    "  \n",
    "  parser.add_argument('--tv_weight', type=float, \n",
    "    default=1e-3,\n",
    "    help='Weight for the total variational loss function. Set small (e.g. 1e-3). (default: %(default)s)')\n",
    "\n",
    "  parser.add_argument('--temporal_weight', type=float, \n",
    "    default=2e2,\n",
    "    help='Weight for the temporal loss function. (default: %(default)s)')\n",
    "\n",
    "  parser.add_argument('--content_loss_function', type=int,\n",
    "    default=1,\n",
    "    choices=[1, 2, 3],\n",
    "    help='Different constants for the content layer loss function. (default: %(default)s)')\n",
    "  \n",
    "  parser.add_argument('--content_layers', nargs='+', type=str, \n",
    "    default=['conv4_2'],\n",
    "    help='VGG19 layers used for the content image. (default: %(default)s)')\n",
    "  \n",
    "  parser.add_argument('--style_layers', nargs='+', type=str,\n",
    "    default=['relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1'],\n",
    "    help='VGG19 layers used for the style image. (default: %(default)s)')\n",
    "  \n",
    "  parser.add_argument('--content_layer_weights', nargs='+', type=float, \n",
    "    default=[1.0], \n",
    "    help='Contributions (weights) of each content layer to loss. (default: %(default)s)')\n",
    "  \n",
    "  parser.add_argument('--style_layer_weights', nargs='+', type=float, \n",
    "    default=[0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "    help='Contributions (weights) of each style layer to loss. (default: %(default)s)')\n",
    "    \n",
    "  parser.add_argument('--original_colors', action='store_true',\n",
    "    help='Transfer the style but not the colors.')\n",
    "\n",
    "  parser.add_argument('--color_convert_type', type=str,\n",
    "    default='yuv',\n",
    "    choices=['yuv', 'ycrcb', 'luv', 'lab'],\n",
    "    help='Color space for conversion to original colors (default: %(default)s)')\n",
    "\n",
    "  parser.add_argument('--color_convert_time', type=str,\n",
    "    default='after',\n",
    "    choices=['after', 'before'],\n",
    "    help='Time (before or after) to convert to original colors (default: %(default)s)')\n",
    "\n",
    "  parser.add_argument('--style_mask', action='store_true',\n",
    "    help='Transfer the style to masked regions.')\n",
    "\n",
    "  parser.add_argument('--style_mask_imgs', nargs='+', type=str, \n",
    "    default=None,\n",
    "    help='Filenames of the style mask images (example: face_mask.png) (default: %(default)s)')\n",
    "  \n",
    "  parser.add_argument('--noise_ratio', type=float, \n",
    "    default=1.0, \n",
    "    help=\"Interpolation value between the content image and noise image if the network is initialized with 'random'.\")\n",
    "\n",
    "  parser.add_argument('--seed', type=int, \n",
    "    default=0,\n",
    "    help='Seed for the random number generator. (default: %(default)s)')\n",
    "  \n",
    "  parser.add_argument('--model_weights', type=str, \n",
    "    default='imagenet-vgg-verydeep-19.mat',\n",
    "    help='Weights and biases of the VGG-19 network.')\n",
    "  \n",
    "  parser.add_argument('--pooling_type', type=str,\n",
    "    default='avg',\n",
    "    choices=['avg', 'max'],\n",
    "    help='Type of pooling in convolutional neural network. (default: %(default)s)')\n",
    "  \n",
    "  parser.add_argument('--device', type=str, \n",
    "    default='/gpu:0',\n",
    "    choices=['/gpu:0', '/cpu:0'],\n",
    "    help='GPU or CPU mode.  GPU mode requires NVIDIA CUDA. (default|recommended: %(default)s)')\n",
    "  \n",
    "  parser.add_argument('--img_output_dir', type=str, \n",
    "    default='./image_output',\n",
    "    help='Relative or absolute directory path to output image and data.')\n",
    "  \n",
    "  # optimizations\n",
    "  parser.add_argument('--optimizer', type=str, \n",
    "    default='lbfgs',\n",
    "    choices=['lbfgs', 'adam'],\n",
    "    help='Loss minimization optimizer.  L-BFGS gives better results.  Adam uses less memory. (default|recommended: %(default)s)')\n",
    "  \n",
    "  parser.add_argument('--learning_rate', type=float, \n",
    "    default=1e0, \n",
    "    help='Learning rate parameter for the Adam optimizer. (default: %(default)s)')\n",
    "  \n",
    "  parser.add_argument('--max_iterations', type=int, \n",
    "    default=50,\n",
    "    help='Max number of iterations for the Adam or L-BFGS optimizer. (default: %(default)s)')\n",
    "\n",
    "  parser.add_argument('--print_iterations', type=int, \n",
    "    default=50,\n",
    "    help='Number of iterations between optimizer print statements. (default: %(default)s)')\n",
    "  \n",
    "  # options for video frames\n",
    "  parser.add_argument('--video', action='store_true', \n",
    "    help='Boolean flag indicating if the user is generating a video.')\n",
    "\n",
    "  parser.add_argument('--start_frame', type=int, \n",
    "  \tdefault=1,\n",
    "    help='First frame number.')\n",
    "  \n",
    "  parser.add_argument('--end_frame', type=int, \n",
    "  \tdefault=1,\n",
    "    help='Last frame number.')\n",
    "  \n",
    "  parser.add_argument('--first_frame_type', type=str,\n",
    "    choices=['random', 'content', 'style'], \n",
    "    default='content',\n",
    "    help='Image used to initialize the network during the rendering of the first frame.')\n",
    "  \n",
    "  parser.add_argument('--init_frame_type', type=str, \n",
    "    choices=['prev_warped', 'prev', 'random', 'content', 'style'], \n",
    "    default='prev_warped',\n",
    "    help='Image used to initialize the network during the every rendering after the first frame.')\n",
    "  \n",
    "  parser.add_argument('--video_input_dir', type=str, \n",
    "    default='./video_input',\n",
    "    help='Relative or absolute directory path to input frames.')\n",
    "  \n",
    "  parser.add_argument('--video_output_dir', type=str, \n",
    "    default='./video_output',\n",
    "    help='Relative or absolute directory path to output frames.')\n",
    "  \n",
    "  parser.add_argument('--content_frame_frmt', type=str, \n",
    "    default='frame_{}.ppm',\n",
    "    help='Filename format of the input content frames.')\n",
    "  \n",
    "  parser.add_argument('--backward_optical_flow_frmt', type=str, \n",
    "    default='backward_{}_{}.flo',\n",
    "    help='Filename format of the backward optical flow files.')\n",
    "  \n",
    "  parser.add_argument('--forward_optical_flow_frmt', type=str, \n",
    "    default='forward_{}_{}.flo',\n",
    "    help='Filename format of the forward optical flow files')\n",
    "  \n",
    "  parser.add_argument('--content_weights_frmt', type=str, \n",
    "    default='reliable_{}_{}.txt',\n",
    "    help='Filename format of the optical flow consistency files.')\n",
    "  \n",
    "  parser.add_argument('--prev_frame_indices', nargs='+', type=int, \n",
    "    default=[1],\n",
    "    help='Previous frames to consider for longterm temporal consistency.')\n",
    "\n",
    "  parser.add_argument('--first_frame_iterations', type=int, \n",
    "    default=2000,\n",
    "    help='Maximum number of optimizer iterations of the first frame. (default: %(default)s)')\n",
    "  \n",
    "  parser.add_argument('--frame_iterations', type=int, \n",
    "    default=800,\n",
    "    help='Maximum number of optimizer iterations for each frame after the first frame. (default: %(default)s)')\n",
    "\n",
    "  #start initiate for ipython \n",
    "    #--img_name\n",
    "  tfName = tempfile.NamedTemporaryFile().name.split(\"/\")[2] \n",
    "  print(tfName)\n",
    "  args = parser.parse_args([\"--content_img\", \"jhlow_q.png\", \"--content_img_dir\", \"./image_input/\",\n",
    "                              \"--style_imgs\",\"starry-night.jpg\",\"--style_imgs_dir\",\"./styles/\",\n",
    "                              \"--device\",\"/cpu:0\",\"--verbose\",\n",
    "                              \"--img_name\",tfName,\n",
    "                              \"--img_output_dir\",\"./image_output/\",\n",
    "                              \"--max_iterations\",\"150\"])  \n",
    "\n",
    "  #args = parser.parse_args()\n",
    "\n",
    "  # normalize weights\n",
    "  args.style_layer_weights   = normalize(args.style_layer_weights)\n",
    "  args.content_layer_weights = normalize(args.content_layer_weights)\n",
    "  args.style_imgs_weights    = normalize(args.style_imgs_weights)\n",
    "\n",
    "  # create directories for output\n",
    "  if args.video:\n",
    "    maybe_make_directory(args.video_output_dir)\n",
    "  else:\n",
    "    maybe_make_directory(args.img_output_dir)\n",
    "\n",
    "  return args\n",
    "\n",
    "'''\n",
    "  pre-trained vgg19 convolutional neural network\n",
    "\n",
    "  remark: layers are manually initialized for clarity.\n",
    "'''\n",
    "vgg19_mean = np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3))\n",
    "\n",
    "def build_vgg19(input_img):\n",
    "  if args.verbose: print('\\nBUILDING VGG-19 NETWORK')\n",
    "  net = {}\n",
    "  _, h, w, d     = input_img.shape\n",
    "  \n",
    "  if args.verbose: print('loading model weights...')\n",
    "  vgg_rawnet     = scipy.io.loadmat(args.model_weights)\n",
    "  vgg_layers     = vgg_rawnet['layers'][0]\n",
    "  if args.verbose: print('constructing layers...')\n",
    "  net['input']   = tf.Variable(np.zeros((1, h, w, d), dtype=np.float32))\n",
    "\n",
    "  if args.verbose: print('LAYER GROUP 1')\n",
    "  net['conv1_1'] = conv_layer('conv1_1', net['input'], W=get_weights(vgg_layers, 0))\n",
    "  net['relu1_1'] = relu_layer('relu1_1', net['conv1_1'], b=get_bias(vgg_layers, 0))\n",
    "\n",
    "  net['conv1_2'] = conv_layer('conv1_2', net['relu1_1'], W=get_weights(vgg_layers, 2))\n",
    "  net['relu1_2'] = relu_layer('relu1_2', net['conv1_2'], b=get_bias(vgg_layers, 2))\n",
    "  \n",
    "  net['pool1']   = pool_layer('pool1', net['relu1_2'])\n",
    "\n",
    "  if args.verbose: print('LAYER GROUP 2')  \n",
    "  net['conv2_1'] = conv_layer('conv2_1', net['pool1'], W=get_weights(vgg_layers, 5))\n",
    "  net['relu2_1'] = relu_layer('relu2_1', net['conv2_1'], b=get_bias(vgg_layers, 5))\n",
    "  \n",
    "  net['conv2_2'] = conv_layer('conv2_2', net['relu2_1'], W=get_weights(vgg_layers, 7))\n",
    "  net['relu2_2'] = relu_layer('relu2_2', net['conv2_2'], b=get_bias(vgg_layers, 7))\n",
    "  \n",
    "  net['pool2']   = pool_layer('pool2', net['relu2_2'])\n",
    "  \n",
    "  if args.verbose: print('LAYER GROUP 3')\n",
    "  net['conv3_1'] = conv_layer('conv3_1', net['pool2'], W=get_weights(vgg_layers, 10))\n",
    "  net['relu3_1'] = relu_layer('relu3_1', net['conv3_1'], b=get_bias(vgg_layers, 10))\n",
    "\n",
    "  net['conv3_2'] = conv_layer('conv3_2', net['relu3_1'], W=get_weights(vgg_layers, 12))\n",
    "  net['relu3_2'] = relu_layer('relu3_2', net['conv3_2'], b=get_bias(vgg_layers, 12))\n",
    "\n",
    "  net['conv3_3'] = conv_layer('conv3_3', net['relu3_2'], W=get_weights(vgg_layers, 14))\n",
    "  net['relu3_3'] = relu_layer('relu3_3', net['conv3_3'], b=get_bias(vgg_layers, 14))\n",
    "\n",
    "  net['conv3_4'] = conv_layer('conv3_4', net['relu3_3'], W=get_weights(vgg_layers, 16))\n",
    "  net['relu3_4'] = relu_layer('relu3_4', net['conv3_4'], b=get_bias(vgg_layers, 16))\n",
    "\n",
    "  net['pool3']   = pool_layer('pool3', net['relu3_4'])\n",
    "\n",
    "  if args.verbose: print('LAYER GROUP 4')\n",
    "  net['conv4_1'] = conv_layer('conv4_1', net['pool3'], W=get_weights(vgg_layers, 19))\n",
    "  net['relu4_1'] = relu_layer('relu4_1', net['conv4_1'], b=get_bias(vgg_layers, 19))\n",
    "\n",
    "  net['conv4_2'] = conv_layer('conv4_2', net['relu4_1'], W=get_weights(vgg_layers, 21))\n",
    "  net['relu4_2'] = relu_layer('relu4_2', net['conv4_2'], b=get_bias(vgg_layers, 21))\n",
    "\n",
    "  net['conv4_3'] = conv_layer('conv4_3', net['relu4_2'], W=get_weights(vgg_layers, 23))\n",
    "  net['relu4_3'] = relu_layer('relu4_3', net['conv4_3'], b=get_bias(vgg_layers, 23))\n",
    "\n",
    "  net['conv4_4'] = conv_layer('conv4_4', net['relu4_3'], W=get_weights(vgg_layers, 25))\n",
    "  net['relu4_4'] = relu_layer('relu4_4', net['conv4_4'], b=get_bias(vgg_layers, 25))\n",
    "\n",
    "  net['pool4']   = pool_layer('pool4', net['relu4_4'])\n",
    "\n",
    "  if args.verbose: print('LAYER GROUP 5')\n",
    "  net['conv5_1'] = conv_layer('conv5_1', net['pool4'], W=get_weights(vgg_layers, 28))\n",
    "  net['relu5_1'] = relu_layer('relu5_1', net['conv5_1'], b=get_bias(vgg_layers, 28))\n",
    "\n",
    "  net['conv5_2'] = conv_layer('conv5_2', net['relu5_1'], W=get_weights(vgg_layers, 30))\n",
    "  net['relu5_2'] = relu_layer('relu5_2', net['conv5_2'], b=get_bias(vgg_layers, 30))\n",
    "\n",
    "  net['conv5_3'] = conv_layer('conv5_3', net['relu5_2'], W=get_weights(vgg_layers, 32))\n",
    "  net['relu5_3'] = relu_layer('relu5_3', net['conv5_3'], b=get_bias(vgg_layers, 32))\n",
    "\n",
    "  net['conv5_4'] = conv_layer('conv5_4', net['relu5_3'], W=get_weights(vgg_layers, 34))\n",
    "  net['relu5_4'] = relu_layer('relu5_4', net['conv5_4'], b=get_bias(vgg_layers, 34))\n",
    "\n",
    "  net['pool5']   = pool_layer('pool5', net['relu5_4'])\n",
    "\n",
    "  return net\n",
    "\n",
    "def conv_layer(layer_name, layer_input, W):\n",
    "  conv = tf.nn.conv2d(layer_input, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "  if args.verbose: print('--{} | shape={} | weights_shape={}'.format(layer_name, \n",
    "    conv.get_shape(), W.get_shape()))\n",
    "  return conv\n",
    "\n",
    "def relu_layer(layer_name, layer_input, b):\n",
    "  relu = tf.nn.relu(layer_input + b)\n",
    "  if args.verbose: \n",
    "    print('--{} | shape={} | bias_shape={}'.format(layer_name, relu.get_shape(), \n",
    "      b.get_shape()))\n",
    "  return relu\n",
    "\n",
    "def pool_layer(layer_name, layer_input):\n",
    "  if args.pooling_type == 'avg':\n",
    "    pool = tf.nn.avg_pool(layer_input, ksize=[1, 2, 2, 1], \n",
    "      strides=[1, 2, 2, 1], padding='SAME')\n",
    "  elif args.pooling_type == 'max':\n",
    "    pool = tf.nn.max_pool(layer_input, ksize=[1, 2, 2, 1], \n",
    "      strides=[1, 2, 2, 1], padding='SAME')\n",
    "  if args.verbose: \n",
    "    print('--{}   | shape={}'.format(layer_name, pool.get_shape()))\n",
    "  return pool\n",
    "\n",
    "def get_weights(vgg_layers, i):\n",
    "  weights = vgg_layers[i][0][0][2][0][0]\n",
    "  W = tf.constant(weights)\n",
    "  return W\n",
    "\n",
    "def get_bias(vgg_layers, i):\n",
    "  bias = vgg_layers[i][0][0][2][0][1]\n",
    "  b = tf.constant(np.reshape(bias, (bias.size)))\n",
    "  return b\n",
    "\n",
    "'''\n",
    "  'a neural algorithm for artistic style' loss functions\n",
    "'''\n",
    "def content_layer_loss(p, x):\n",
    "  _, h, w, d = p.get_shape()\n",
    "  M = h.value * w.value\n",
    "  N = d.value\n",
    "  if args.content_loss_function   == 1:\n",
    "    K = 1. / (2. * N**0.5 * M**0.5)\n",
    "  elif args.content_loss_function == 2:\n",
    "    K = 1. / (N * M)\n",
    "  elif args.content_loss_function == 3:  \n",
    "    K = 1. / 2.\n",
    "  loss = K * tf.reduce_sum(tf.pow((x - p), 2))\n",
    "  return loss\n",
    "\n",
    "def style_layer_loss(a, x):\n",
    "  _, h, w, d = a.get_shape()\n",
    "  M = h.value * w.value\n",
    "  N = d.value\n",
    "  A = gram_matrix(a, M, N)\n",
    "  G = gram_matrix(x, M, N)\n",
    "  loss = (1./(4 * N**2 * M**2)) * tf.reduce_sum(tf.pow((G - A), 2))\n",
    "  return loss\n",
    "\n",
    "def gram_matrix(x, area, depth):\n",
    "  F = tf.reshape(x, (area, depth))\n",
    "  G = tf.matmul(tf.transpose(F), F)\n",
    "  return G\n",
    "\n",
    "def mask_style_layer(a, x, mask_img):\n",
    "  _, h, w, d = a.get_shape()\n",
    "  mask = get_mask_image(mask_img, w.value, h.value)\n",
    "  mask = tf.convert_to_tensor(mask)\n",
    "  tensors = []\n",
    "  for _ in range(d.value): \n",
    "    tensors.append(mask)\n",
    "  mask = tf.stack(tensors, axis=2)\n",
    "  mask = tf.stack(mask, axis=0)\n",
    "  mask = tf.expand_dims(mask, 0)\n",
    "  a = tf.mul(a, mask)\n",
    "  x = tf.mul(x, mask)\n",
    "  return a, x\n",
    "\n",
    "def sum_masked_style_losses(sess, net, style_imgs):\n",
    "  total_style_loss = 0.\n",
    "  weights = args.style_imgs_weights\n",
    "  masks = args.style_mask_imgs\n",
    "  for img, img_weight, img_mask in zip(style_imgs, weights, masks):\n",
    "    sess.run(net['input'].assign(img))\n",
    "    style_loss = 0.\n",
    "    for layer, weight in zip(args.style_layers, args.style_layer_weights):\n",
    "      a = sess.run(net[layer])\n",
    "      x = net[layer]\n",
    "      a = tf.convert_to_tensor(a)\n",
    "      a, x = mask_style_layer(a, x, img_mask)\n",
    "      style_loss += style_layer_loss(a, x) * weight\n",
    "    style_loss /= float(len(args.style_layers))\n",
    "    total_style_loss += (style_loss * img_weight)\n",
    "  total_style_loss /= float(len(style_imgs))\n",
    "  return total_style_loss\n",
    "\n",
    "def sum_style_losses(sess, net, style_imgs):\n",
    "  total_style_loss = 0.\n",
    "  weights = args.style_imgs_weights\n",
    "  for img, img_weight in zip(style_imgs, weights):\n",
    "    sess.run(net['input'].assign(img))\n",
    "    style_loss = 0.\n",
    "    for layer, weight in zip(args.style_layers, args.style_layer_weights):\n",
    "      a = sess.run(net[layer])\n",
    "      x = net[layer]\n",
    "      a = tf.convert_to_tensor(a)\n",
    "      style_loss += style_layer_loss(a, x) * weight\n",
    "    style_loss /= float(len(args.style_layers))\n",
    "    total_style_loss += (style_loss * img_weight)\n",
    "  total_style_loss /= float(len(style_imgs))\n",
    "  return total_style_loss\n",
    "\n",
    "def sum_content_losses(sess, net, content_img):\n",
    "  sess.run(net['input'].assign(content_img))\n",
    "  content_loss = 0.\n",
    "  for layer, weight in zip(args.content_layers, args.content_layer_weights):\n",
    "    p = sess.run(net[layer])\n",
    "    x = net[layer]\n",
    "    p = tf.convert_to_tensor(p)\n",
    "    content_loss += content_layer_loss(p, x) * weight\n",
    "  content_loss /= float(len(args.content_layers))\n",
    "  return content_loss\n",
    "\n",
    "'''\n",
    "  'artistic style transfer for videos' loss functions\n",
    "'''\n",
    "def temporal_loss(x, w, c):\n",
    "  c = c[np.newaxis,:,:,:]\n",
    "  D = float(x.size)\n",
    "  loss = (1. / D) * tf.reduce_sum(c * tf.nn.l2_loss(x - w))\n",
    "  loss = tf.cast(loss, tf.float32)\n",
    "  return loss\n",
    "\n",
    "def get_longterm_weights(i, j):\n",
    "  c_sum = 0.\n",
    "  for k in range(args.prev_frame_indices):\n",
    "    if i - k > i - j:\n",
    "      c_sum += get_content_weights(i, i - k)\n",
    "  c = get_content_weights(i, i - j)\n",
    "  c_max = tf.maximum(c - c_sum, 0.)\n",
    "  return c_max\n",
    "\n",
    "def sum_longterm_temporal_losses(sess, net, frame, input_img):\n",
    "  x = sess.run(net['input'].assign(input_img))\n",
    "  loss = 0.\n",
    "  for j in range(args.prev_frame_indices):\n",
    "    prev_frame = frame - j\n",
    "    w = get_prev_warped_frame(frame)\n",
    "    c = get_longterm_weights(frame, prev_frame)\n",
    "    loss += temporal_loss(x, w, c)\n",
    "  return loss\n",
    "\n",
    "def sum_shortterm_temporal_losses(sess, net, frame, input_img):\n",
    "  x = sess.run(net['input'].assign(input_img))\n",
    "  prev_frame = frame - 1\n",
    "  w = get_prev_warped_frame(frame)\n",
    "  c = get_content_weights(frame, prev_frame)\n",
    "  loss = temporal_loss(x, w, c)\n",
    "  return loss\n",
    "\n",
    "'''\n",
    "  denoising loss function\n",
    "\n",
    "  remark: not sure this does anything significant.\n",
    "'''\n",
    "def sum_total_variation_losses(sess, net, input_img):\n",
    "  b, h, w, d = input_img.shape\n",
    "  x = net['input']\n",
    "  tv_y_size = b * (h-1) * w * d\n",
    "  tv_x_size = b * h * (w-1) * d\n",
    "  loss_y = tf.nn.l2_loss(x[:,1:,:,:] - x[:,:-1,:,:]) \n",
    "  loss_y /= tv_y_size\n",
    "  loss_x = tf.nn.l2_loss(x[:,:,1:,:] - x[:,:,:-1,:]) \n",
    "  loss_x /= tv_x_size\n",
    "  loss = 2 * (loss_y + loss_x)\n",
    "  loss = tf.cast(loss, tf.float32)\n",
    "  return loss\n",
    "\n",
    "'''\n",
    "  utilities and i/o\n",
    "'''\n",
    "def read_image(path):\n",
    "  # bgr image\n",
    "  img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "  check_image(img, path)\n",
    "  img = img.astype(np.float32)\n",
    "  img = preprocess(img, vgg19_mean)\n",
    "  return img\n",
    "\n",
    "def write_image(path, img):\n",
    "  img = postprocess(img, vgg19_mean)\n",
    "  cv2.imwrite(path, img)\n",
    "\n",
    "def preprocess(img, mean):\n",
    "  # bgr to rgb\n",
    "  img = img[...,::-1]\n",
    "  # shape (h, w, d) to (1, h, w, d)\n",
    "  img = img[np.newaxis,:,:,:]\n",
    "  img -= mean\n",
    "  return img\n",
    "\n",
    "def postprocess(img, mean):\n",
    "  img += mean\n",
    "  # shape (1, h, w, d) to (h, w, d)\n",
    "  img = img[0]\n",
    "  img = np.clip(img, 0, 255).astype('uint8')\n",
    "  # rgb to bgr\n",
    "  img = img[...,::-1]\n",
    "  return img\n",
    "\n",
    "def read_flow_file(path):\n",
    "  with open(path, 'rb') as f:\n",
    "    # 4 bytes header\n",
    "    header = struct.unpack('4s', f.read(4))[0]\n",
    "    # 4 bytes width, height    \n",
    "    w = struct.unpack('i', f.read(4))[0]\n",
    "    h = struct.unpack('i', f.read(4))[0]   \n",
    "    flow = np.ndarray((2, h, w), dtype=np.float32)\n",
    "    for y in range(h):\n",
    "      for x in range(w):\n",
    "        flow[1,y,x] = struct.unpack('f', f.read(4))[0]\n",
    "        flow[0,y,x] = struct.unpack('f', f.read(4))[0]\n",
    "  return flow\n",
    "\n",
    "def read_weights_file(path):\n",
    "  lines = open(path).readlines()\n",
    "  header = list(map(int, lines[0].split(' ')))\n",
    "  w = header[0]\n",
    "  h = header[1]\n",
    "  vals = np.zeros((h, w), dtype=np.float32)\n",
    "  for i in range(1, len(lines)):\n",
    "    line = lines[i].rstrip().split(' ')\n",
    "    vals[i-1] = np.array(list(map(np.float32, line)))\n",
    "    vals[i-1] = list(map(lambda x: 0. if x < 255. else 1., vals[i-1]))\n",
    "  # expand to 3 channels\n",
    "  weights = np.dstack([vals.astype(np.float32)] * 3)\n",
    "  return weights\n",
    "\n",
    "def normalize(weights):\n",
    "  denom = sum(weights)\n",
    "  if denom > 0.:\n",
    "    return [float(i) / denom for i in weights]\n",
    "  else: return [0.] * len(weights)\n",
    "\n",
    "def maybe_make_directory(dir_path):\n",
    "  if not os.path.exists(dir_path):  \n",
    "    os.makedirs(dir_path)\n",
    "\n",
    "def check_image(img, path):\n",
    "  if img is None:\n",
    "    raise OSError(errno.ENOENT, \"No such file\", path)\n",
    "\n",
    "'''\n",
    "  rendering -- where the magic happens\n",
    "'''\n",
    "def stylize(content_img, style_imgs, init_img, frame=None):\n",
    "  with tf.device(args.device), tf.Session() as sess:\n",
    "    # setup network\n",
    "    net = build_vgg19(content_img)\n",
    "    \n",
    "    # style loss\n",
    "    if args.style_mask:\n",
    "      L_style = sum_masked_style_losses(sess, net, style_imgs)\n",
    "    else:\n",
    "      L_style = sum_style_losses(sess, net, style_imgs)\n",
    "    \n",
    "    # content loss\n",
    "    L_content = sum_content_losses(sess, net, content_img)\n",
    "    \n",
    "    # denoising loss\n",
    "    L_tv = sum_total_variation_losses(sess, net, init_img)\n",
    "    \n",
    "    # loss weights\n",
    "    alpha = args.content_weight\n",
    "    beta  = args.style_weight\n",
    "    theta = args.tv_weight\n",
    "    \n",
    "    # total loss\n",
    "    L_total  = alpha * L_content\n",
    "    L_total += beta  * L_style\n",
    "    L_total += theta * L_tv\n",
    "    \n",
    "    # video temporal loss\n",
    "    if args.video and frame > 1:\n",
    "      gamma      = args.temporal_weight\n",
    "      L_temporal = sum_shortterm_temporal_losses(sess, net, frame, init_img)\n",
    "      L_total   += gamma * L_temporal\n",
    "\n",
    "    # optimization algorithm\n",
    "    optimizer = get_optimizer(L_total)\n",
    "\n",
    "    if args.optimizer == 'adam':\n",
    "      minimize_with_adam(sess, net, optimizer, init_img, L_total)\n",
    "    elif args.optimizer == 'lbfgs':\n",
    "      minimize_with_lbfgs(sess, net, optimizer, init_img)\n",
    "    \n",
    "    output_img = sess.run(net['input'])\n",
    "    \n",
    "    if args.original_colors:\n",
    "      output_img = convert_to_original_colors(np.copy(content_img), output_img)\n",
    "\n",
    "    if args.video:\n",
    "      write_video_output(frame, output_img)\n",
    "    else:\n",
    "      write_image_output(output_img, content_img, style_imgs, init_img)\n",
    "\n",
    "def minimize_with_lbfgs(sess, net, optimizer, init_img):\n",
    "  if args.verbose: print('\\nMINIMIZING LOSS USING: L-BFGS OPTIMIZER')\n",
    "  init_op = tf.initialize_all_variables()\n",
    "  sess.run(init_op)\n",
    "  sess.run(net['input'].assign(init_img))\n",
    "  optimizer.minimize(sess)\n",
    "\n",
    "def minimize_with_adam(sess, net, optimizer, init_img, loss):\n",
    "  if args.verbose: print('\\nMINIMIZING LOSS USING: ADAM OPTIMIZER')\n",
    "  train_op = optimizer.minimize(loss)\n",
    "  init_op = tf.initialize_all_variables()\n",
    "  sess.run(init_op)\n",
    "  sess.run(net['input'].assign(init_img))\n",
    "  iterations = 0\n",
    "  while (iterations < args.max_iterations):\n",
    "    sess.run(train_op)\n",
    "    if iterations % args.print_iterations == 0 and args.verbose:\n",
    "      curr_loss = loss.eval()\n",
    "      print(\"At iterate {}\\tf=  {:.5E}\".format(iterations, curr_loss))\n",
    "    iterations += 1\n",
    "\n",
    "def get_optimizer(loss):\n",
    "  print_iterations = args.print_iterations if args.verbose else 0\n",
    "  if args.optimizer == 'lbfgs':\n",
    "    optimizer = tf.contrib.opt.ScipyOptimizerInterface(\n",
    "      loss, method='L-BFGS-B',\n",
    "      options={'maxiter': args.max_iterations,\n",
    "                  'disp': print_iterations})\n",
    "  elif args.optimizer == 'adam':\n",
    "    optimizer = tf.train.AdamOptimizer(args.learning_rate)\n",
    "  return optimizer\n",
    "\n",
    "def write_video_output(frame, output_img):\n",
    "  fn = args.content_frame_frmt.format(str(frame).zfill(4))\n",
    "  path = os.path.join(args.video_output_dir, fn)\n",
    "  write_image(path, output_img)\n",
    "\n",
    "def write_image_output(output_img, content_img, style_imgs, init_img):\n",
    "  out_dir = os.path.join(args.img_output_dir, args.img_name)\n",
    "  maybe_make_directory(out_dir)\n",
    "  img_path = os.path.join(out_dir, args.img_name+'.png')\n",
    "  content_path = os.path.join(out_dir, 'content.png')\n",
    "  init_path = os.path.join(out_dir, 'init.png')\n",
    "\n",
    "  write_image(img_path, output_img)\n",
    "  write_image(content_path, content_img)\n",
    "  write_image(init_path, init_img)\n",
    "  index = 0\n",
    "  for style_img in style_imgs:\n",
    "    path = os.path.join(out_dir, 'style_'+str(index)+'.png')\n",
    "    write_image(path, style_img)\n",
    "    index += 1\n",
    "  \n",
    "  # save the configuration settings\n",
    "  out_file = os.path.join(out_dir, 'meta_data.txt')\n",
    "  f = open(out_file, 'w')\n",
    "  f.write('image_name: {}\\n'.format(args.img_name))\n",
    "  f.write('content: {}\\n'.format(args.content_img))\n",
    "  index = 0\n",
    "  for style_img, weight in zip(args.style_imgs, args.style_imgs_weights):\n",
    "    f.write('styles['+str(index)+']: {} * {}\\n'.format(weight, style_img))\n",
    "    index += 1\n",
    "  index = 0\n",
    "  if args.style_mask_imgs is not None:\n",
    "    for mask in args.style_mask_imgs:\n",
    "      f.write('style_masks['+str(index)+']: {}\\n'.format(mask))\n",
    "      index += 1\n",
    "  f.write('init_type: {}\\n'.format(args.init_img_type))\n",
    "  f.write('content_weight: {}\\n'.format(args.content_weight))\n",
    "  f.write('style_weight: {}\\n'.format(args.style_weight))\n",
    "  f.write('tv_weight: {}\\n'.format(args.tv_weight))\n",
    "  f.write('content_layers: {}\\n'.format(args.content_layers))\n",
    "  f.write('style_layers: {}\\n'.format(args.style_layers))\n",
    "  f.write('optimizer_type: {}\\n'.format(args.optimizer))\n",
    "  f.write('max_iterations: {}\\n'.format(args.max_iterations))\n",
    "  f.write('max_image_size: {}\\n'.format(args.max_size))\n",
    "  f.close()\n",
    "\n",
    "'''\n",
    "  image loading and processing\n",
    "'''\n",
    "def get_init_image(init_type, content_img, style_imgs, frame=None):\n",
    "  if init_type == 'content':\n",
    "    return content_img\n",
    "  elif init_type == 'style':\n",
    "    return style_imgs[0]\n",
    "  elif init_type == 'random':\n",
    "    init_img = get_noise_image(args.noise_ratio, content_img)\n",
    "    return init_img\n",
    "  # only for video frames\n",
    "  elif init_type == 'prev':\n",
    "    init_img = get_prev_frame(frame)\n",
    "    return init_img\n",
    "  elif init_type == 'prev_warped':\n",
    "    init_img = get_prev_warped_frame(frame)\n",
    "    return init_img\n",
    "\n",
    "def get_content_frame(frame):\n",
    "  fn = args.content_frame_frmt.format(str(frame).zfill(4))\n",
    "  path = os.path.join(args.video_input_dir, fn)\n",
    "  img = read_image(path)\n",
    "  return img\n",
    "\n",
    "def get_content_image(content_img):\n",
    "  path = os.path.join(args.content_img_dir, content_img)\n",
    "   # bgr image\n",
    "  img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "  check_image(img, path)\n",
    "  img = img.astype(np.float32)\n",
    "  h, w, d = img.shape\n",
    "  mx = args.max_size\n",
    "  # resize if > max size\n",
    "  if h > w and h > mx:\n",
    "    w = (float(mx) / float(h)) * w\n",
    "    img = cv2.resize(img, dsize=(int(w), mx), interpolation=cv2.INTER_AREA)\n",
    "  if w > mx:\n",
    "    h = (float(mx) / float(w)) * h\n",
    "    img = cv2.resize(img, dsize=(mx, int(h)), interpolation=cv2.INTER_AREA)\n",
    "  img = preprocess(img, vgg19_mean)\n",
    "  return img\n",
    "\n",
    "def get_style_images(content_img):\n",
    "  _, ch, cw, cd = content_img.shape\n",
    "  style_imgs = []\n",
    "  for style_fn in args.style_imgs:\n",
    "    path = os.path.join(args.style_imgs_dir, style_fn)\n",
    "    # bgr image\n",
    "    img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    check_image(img, path)\n",
    "    img = img.astype(np.float32)\n",
    "    img = cv2.resize(img, dsize=(cw, ch), interpolation=cv2.INTER_AREA)\n",
    "    img = preprocess(img, vgg19_mean)\n",
    "    style_imgs.append(img)\n",
    "  return style_imgs\n",
    "\n",
    "def get_noise_image(noise_ratio, content_img):\n",
    "  np.random.seed(args.seed)\n",
    "  noise_img = np.random.uniform(-20., 20., content_img.shape).astype(np.float32)\n",
    "  img = noise_ratio * noise_img + (1.-noise_ratio) * content_img\n",
    "  return img\n",
    "\n",
    "def get_mask_image(mask_img, width, height):\n",
    "  path = os.path.join(args.content_img_dir, mask_img)\n",
    "  img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "  check_image(img, path)\n",
    "  img = cv2.resize(img, dsize=(width, height), interpolation=cv2.INTER_AREA)\n",
    "  img = img.astype(np.float32)\n",
    "  mx = np.amax(img)\n",
    "  img /= mx\n",
    "  return img\n",
    "\n",
    "def get_prev_frame(frame):\n",
    "  # previously stylized frame\n",
    "  prev_frame = frame - 1\n",
    "  fn = args.content_frame_frmt.format(str(prev_frame).zfill(4))\n",
    "  path = os.path.join(args.video_output_dir, fn)\n",
    "  img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "  check_image(img, path)\n",
    "  return img\n",
    "\n",
    "def get_prev_warped_frame(frame):\n",
    "  prev_img = get_prev_frame(frame)\n",
    "  prev_frame = frame - 1\n",
    "  # backwards flow: current frame -> previous frame\n",
    "  fn = args.backward_optical_flow_frmt.format(str(frame), str(prev_frame))\n",
    "  path = os.path.join(args.video_input_dir, fn)\n",
    "  flow = read_flow_file(path)\n",
    "  warped_img = warp_image(prev_img, flow).astype(np.float32)\n",
    "  img = preprocess(warped_img, vgg19_mean)\n",
    "  return img\n",
    "\n",
    "def get_content_weights(frame, prev_frame):\n",
    "  forward_fn = args.content_weights_frmt.format(str(prev_frame), str(frame))\n",
    "  backward_fn = args.content_weights_frmt.format(str(frame), str(prev_frame))\n",
    "  forward_path = os.path.join(args.video_input_dir, forward_fn)\n",
    "  backward_path = os.path.join(args.video_input_dir, backward_fn)\n",
    "  forward_weights = read_weights_file(forward_path)\n",
    "  backward_weights = read_weights_file(backward_path)\n",
    "  return forward_weights #, backward_weights\n",
    "\n",
    "def warp_image(src, flow):\n",
    "  _, h, w = flow.shape\n",
    "  flow_map = np.zeros(flow.shape, dtype=np.float32)\n",
    "  for y in range(h):\n",
    "    flow_map[1,y,:] = float(y) + flow[1,y,:]\n",
    "  for x in range(w):\n",
    "    flow_map[0,:,x] = float(x) + flow[0,:,x]\n",
    "  # remap pixels to optical flow\n",
    "  dst = cv2.remap(\n",
    "    src, flow_map[0], flow_map[1], \n",
    "    interpolation=cv2.INTER_CUBIC, borderMode=cv2.BORDER_TRANSPARENT)\n",
    "  return dst\n",
    "\n",
    "def convert_to_original_colors(content_img, stylized_img):\n",
    "  content_img  = postprocess(content_img, vgg19_mean)\n",
    "  stylized_img = postprocess(stylized_img, vgg19_mean)\n",
    "  if args.color_convert_type == 'yuv':\n",
    "    cvt_type = cv2.COLOR_BGR2YUV\n",
    "    inv_cvt_type = cv2.COLOR_YUV2BGR\n",
    "  elif args.color_convert_type == 'ycrcb':\n",
    "    cvt_type = cv2.COLOR_BGR2YCR_CB\n",
    "    inv_cvt_type = cv2.COLOR_YCR_CB2BGR\n",
    "  elif args.color_convert_type == 'luv':\n",
    "    cvt_type = cv2.COLOR_BGR2LUV\n",
    "    inv_cvt_type = cv2.COLOR_LUV2BGR\n",
    "  elif args.color_convert_type == 'lab':\n",
    "    cvt_type = cv2.COLOR_BGR2LAB\n",
    "    inv_cvt_type = cv2.COLOR_LAB2BGR\n",
    "  content_cvt = cv2.cvtColor(content_img, cvt_type)\n",
    "  stylized_cvt = cv2.cvtColor(stylized_img, cvt_type)\n",
    "  c1, _, _ = cv2.split(stylized_cvt)\n",
    "  _, c2, c3 = cv2.split(content_cvt)\n",
    "  merged = cv2.merge((c1, c2, c3))\n",
    "  dst = cv2.cvtColor(merged, inv_cvt_type).astype(np.float32)\n",
    "  dst = preprocess(dst, vgg19_mean)\n",
    "  return dst\n",
    "\n",
    "def render_single_image():\n",
    "  content_img = get_content_image(args.content_img)\n",
    "  style_imgs = get_style_images(content_img)\n",
    "  with tf.Graph().as_default():\n",
    "    print('\\n---- RENDERING SINGLE IMAGE ----\\n')\n",
    "    init_img = get_init_image(args.init_img_type, content_img, style_imgs)\n",
    "    tick = time.time()\n",
    "    stylize(content_img, style_imgs, init_img)\n",
    "    tock = time.time()\n",
    "    print('Single image elapsed time: {}'.format(tock - tick))\n",
    "\n",
    "def render_video():\n",
    "  for frame in range(args.start_frame, args.end_frame+1):\n",
    "    with tf.Graph().as_default():\n",
    "      print('\\n---- RENDERING VIDEO FRAME: {}/{} ----\\n'.format(frame, args.end_frame))\n",
    "      if frame == 1:\n",
    "        content_frame = get_content_frame(frame)\n",
    "        style_imgs = get_style_images(content_frame)\n",
    "        init_img = get_init_image(args.first_frame_type, content_frame, style_imgs, frame)\n",
    "        args.max_iterations = args.first_frame_iterations\n",
    "        tick = time.time()\n",
    "        stylize(content_frame, style_imgs, init_img, frame)\n",
    "        tock = time.time()\n",
    "        print('Frame {} elapsed time: {}'.format(frame, tock - tick))\n",
    "      else:\n",
    "        content_frame = get_content_frame(frame)\n",
    "        style_imgs = get_style_images(content_frame)\n",
    "        init_img = get_init_image(args.init_frame_type, content_frame, style_imgs, frame)\n",
    "        args.max_iterations = args.frame_iterations\n",
    "        tick = time.time()\n",
    "        stylize(content_frame, style_imgs, init_img, frame)\n",
    "        tock = time.time()\n",
    "        print('Frame {} elapsed time: {}'.format(frame, tock - tick))\n",
    "\n",
    "def main():\n",
    "  global args\n",
    "  args = parse_args()\n",
    "  if args.video: render_video()\n",
    "  else: render_single_image()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
